{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**General Linear Model**\n",
        "\n",
        "1. What is the purpose of the General Linear Model (GLM)?\n",
        "----->\n",
        " GLM models allow us to build a linear relationship between the response and predictors, even though their underlying relationship is not linear. This is made possible by using a link function, which links the response variable to a linear model.\n",
        "\n",
        "\n",
        " 2. What are the key assumptions of the General Linear Model?\n",
        "----->\n",
        "The general linear model fitted using ordinary least squares (which includes Student's t test, ANOVA, and linear regression) makes four assumptions: linearity, homoskedasticity (constant variance), normality, and independence.\n",
        "\n",
        "\n",
        "3. How do you interpret the coefficients in a GLM?\n",
        "The GLM coefficients only show the multiplicative change in odds ratio. so if p1 is the risk of getting a high score for black defendants and p0 is the risk of getting a high score for white defendants, then exp(0.47721) shows (p1/(1-p1))/(p0/(1-p0))\n",
        "\n",
        "\n",
        "4. What is the difference between a univariate and multivariate GLM?\n",
        "----->\n",
        "The most basic difference is that univariate regression has one explanatory (predictor) variable x and multivariate regression has more at least two explanatory (predictor) variables x1,x2,...,xn .\n",
        "\n",
        "\n",
        "5. Explain the concept of interaction effects in a GLM.\n",
        "----->\n",
        "In general, the existence of an interaction means that the effect of one variable depends on the value of the other variable with which it interacts. If there isn't an interaction, then the value of the other variable doesn't matter\n",
        "This is easiest to understand in the case of linear regression.\n",
        "\n",
        "\n",
        "6. How do you handle categorical predictors in a GLM?\n",
        "---->\n",
        "Drop Categorical Variables The easiest approach to dealing with categorical variables is to simply remove them from the dataset. This approach will only work well if the columns did not contain useful information.\n",
        "\n",
        "Label Encoding Label encoding assigns each unique value to a different integer.\n",
        "\n",
        "\n",
        "7. What is the purpose of the design matrix in a GLM?\n",
        "------>\n",
        "The purpose of the design matrix is to allow models that further constrain parameter sets. There are as many columns in a block as there are degrees of freedom for the term. The first block is for the constant and contains one column, a column of all ones.\n",
        "\n",
        "\n",
        "8. How do you test the significance of predictors in a GLM?\n",
        "---->\n",
        "A low p-value (< 0.05) indicates that you can reject the null hypothesis.\n",
        "\n",
        "The F-Test of overall significance in regression\n",
        "\n",
        "\n",
        "9. What is the difference between Type I, Type II, and Type III sums of squares in a GLM?\n",
        "----->\n",
        "Type I sum of squares are “sequential.”  In essence the factors are tested in the order they are listed in the model.\n",
        "\n",
        "\n",
        "\n",
        "Type III sum of squares are “partial.”  In essence, every term in the model is tested in light of every other term in the model.  That means that main effects are tested in light of interaction terms as well as in light of other main effects.\n",
        "\n",
        "\n",
        "\n",
        "Type II sum of squares are similar to Type III, except that they preserve the principle of marginality.  This means that main factors are tested in light of one another, but not in light of the interaction term.\n",
        "\n",
        "\n",
        "10. Explain the concept of deviance in a GLM.\n",
        "---->\n",
        "The deviance is used to compare two models – in particular in the case of generalized linear models (GLM) where it has a similar role to residual sum of squares from ANOVA in linear models (RSS).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rxDcrh1IXF-3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Regression:**\n",
        "\n",
        "11. What is regression analysis and what is its purpose?\n",
        "---->\n",
        "Regression analysis is a powerful statistical method that allows you to examine the relationship between two or more variables of interest. While there are many types of regression analysis, at their core they all examine the influence of one or more independent variables on a dependent variable.\n",
        "\n",
        "\n",
        "12. What is the difference between simple linear regression and multiple linear regression?\n",
        "----->\n",
        "Multiple regression is a broader class of regressions that encompasses linear and nonlinear regressions with multiple explanatory variables. Whereas linear regress only has one independent variable impacting the slope of the relationship, multiple regression incorporates multiple independent variables.\n",
        "\n",
        "\n",
        "13. How do you interpret the R-squared value in regression?\n",
        "---->\n",
        "In linear regression models, r squared interpretation is a goodness-fit-measure. It takes into account the strength of the relationship between the model and the dependent variable. Its convenience is measured on a scale of 0 – 100%.\n",
        "\n",
        "\n",
        "14. What is the difference between correlation and regression?\n",
        "------>\n",
        "'Correlation', as the name says, it determines the interconnection or a co-relationship between the variables. 'Regression' explains how an independent variable is numerically associated with the dependent variable. In Correlation, both the independent and dependent values have no difference.\n",
        "\n",
        "\n",
        "15. What is the difference between the coefficients and the intercept in regression?\n",
        "---->\n",
        "The simple linear regression model is essentially a linear equation of the form y = c + b*x; where y is the dependent variable (outcome), x is the independent variable (predictor), b is the slope of the line; also known as regression coefficient and c is the intercept; labeled as constant.\n",
        "\n",
        "\n",
        "16. How do you handle outliers in regression analysis?\n",
        "----->\n",
        "There are many possible approaches to dealing with outliers: removing them from the observations, treating them (for example, capping the extreme observations at a reasonable value), or using algorithms that are well-suited for dealing with such values on their own.\n",
        "\n",
        "\n",
        "17. What is the difference between ridge regression and ordinary least squares regression?\n",
        "------>\n",
        "Ridge regression is a term used to refer to a linear regression model whose coefficients are estimated not by ordinary least squares (OLS), but by an estimator, called ridge estimator, that, albeit biased, has lower variance than the OLS estimator.\n",
        "\n",
        "\n",
        "18. What is heteroscedasticity in regression and how does it affect the model?\n",
        "----->\n",
        "Heteroskedasticity refers to situations where the variance of the residuals is unequal over a range of measured values. When running a regression analysis, heteroskedasticity results in an unequal scatter of the residuals (also known as the error term)\n",
        "\n",
        "\n",
        "19. How do you handle multicollinearity in regression analysis?\n",
        "----->\n",
        "Remove some of the highly correlated independent variables.\n",
        "\n",
        "Linearly combine the independent variables, such as adding them together.\n",
        "\n",
        "Partial least squares regression uses principal component analysis to create a set of uncorrelated components to include in the model.\n",
        "\n",
        "20. What is polynomial regression and when is it used?\n",
        "------>\n",
        "A polynomial regression model is a machine learning model that can capture non-linear relationships between variables by fitting a non-linear regression line, which may not be possible with simple linear regression. It is used when linear regression models may not adequately capture the complexity of the relationship.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QdrAHvNKXKFW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loss function:**\n",
        "\n",
        "21. What is a loss function and what is its purpose in machine learning?\n",
        "------->\n",
        "At its core, a loss function is a measure of how good your prediction model does in terms of being able to predict the expected outcome(or value). We convert the learning problem into an optimization problem, define a loss function and then optimize the algorithm to minimize the loss function.\n",
        "\n",
        "\n",
        "22. What is the difference between a convex and non-convex loss function?\n",
        "----->\n",
        "A convex function is one in which a line drawn between any two points on the graph lies on the graph or above it. There is only one requirement. A non-convex function is one in which a line drawn between any two points on the graph may cross additional points. It was described as “wavy.”\n",
        "\n",
        "\n",
        "23. What is mean squared error (MSE) and how is it calculated?\n",
        "----->\n",
        "The Mean Squared Error measures how close a regression line is to a set of data points. It is a risk function corresponding to the expected value of the squared error loss. Mean square error is calculated by taking the average, specifically the mean, of errors squared from data as it relates to a function.\n",
        "\n",
        "\n",
        "24. What is mean absolute error (MAE) and how is it calculated?\n",
        "------>\n",
        "MAE is calculated as the sum of absolute errors divided by the sample size: It is thus an arithmetic average of the absolute errors , where is the prediction and. the true value. Alternative formulations may include relative frequencies as weight factors.\n",
        "\n",
        "\n",
        "25. What is log loss (cross-entropy loss) and how is it calculated?\n",
        "------>\n",
        "Also called logarithmic loss, log loss or logistic loss. Each predicted class probability is compared to the actual class desired output 0 or 1 and a score/loss is calculated that penalizes the probability based on how far it is from the actual expected value.\n",
        "\n",
        "\n",
        "26. How do you choose the appropriate loss function for a given problem?\n",
        "------->\n",
        "Mean Squared error.\n",
        "Mean Absolute Error.\n",
        "Log-Likelihood Loss.\n",
        "Hinge Loss.\n",
        "Huber Loss.\n",
        "\n",
        "\n",
        "27. Explain the concept of regularization in the context of loss functions.\n",
        "---->\n",
        "What is the regularization term in loss function?\n",
        "During the L2 regularization the loss function of the neural network as extended by a so-called regularization term, which is called here Ω. The regularization term Ω is defined as the Euclidean Norm (or L2 norm) of the weight matrices, which is the sum over all squared weight values of a weight matrix.\n",
        "\n",
        "\n",
        "\n",
        "28. What is Huber loss and how does it handle outliers?\n",
        "----->\n",
        "\n",
        "The Huber loss is the convolution of the absolute value function with the rectangular function, scaled and translated. Thus it \"smoothens out\" the former's corner at the orgin.\n",
        "\n",
        "The huber loss function essentially has characteristics of both Mean Squared Error and Mean Absolute Error. However, huber loss functions are less sensitive to outliers than Mean Squared Error, giving a more accurate prediction or estimation.\n",
        "\n",
        "\n",
        "29. What is quantile loss and when is it used?\n",
        "----->\n",
        "a flexible loss function that can be incorporated into any regression model to predict a certain variable quantile. Based on the example of LightGBM, we saw how to adjust a model, so it solves a quantile regression problem.\n",
        "\n",
        "\n",
        "\n",
        "30. What is the difference between squared loss and absolute loss?\n",
        "-------->\n",
        "For square loss, you will choose the estimated mean of y0, as the true mean minimizes square loss on average (where the average is taken across random samples of y0 subject to x=x0). For absolute loss, you will choose the estimated median"
      ],
      "metadata": {
        "id": "zuEk69AFd5uD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optimizer (GD):**\n",
        "\n",
        "31. What is an optimizer and what is its purpose in machine learning?\n",
        "--------->\n",
        "An optimizer is an algorithm or function that adapts the neural network's attributes, like learning rate and weights. Hence, it assists in improving the accuracy and reduces the total loss.\n",
        "\n",
        "\n",
        "32. What is Gradient Descent (GD) and how does it work?\n",
        "----->\n",
        "Gradient Descent is the most common optimization algorithm in machine learning and deep learning. It is a first-order optimization algorithm. This means it only takes into account the first derivative when performing the updates on the parameters\n",
        "\n",
        "\n",
        "\n",
        "33. What are the different variations of Gradient Descent?\n",
        "----->\n",
        "Three simple variants of gradient descent algorithms, namely batch gradient descent, stochastic gradient descent and mini-batch gradient descent are compared in this experiment.\n",
        "\n",
        "\n",
        "\n",
        "34. What is the learning rate in GD and how do you choose an appropriate value?\n",
        "------>\n",
        "The learning rate hyperparameter controls the rate or speed at which the model learns. Specifically, it controls the amount of apportioned error that the weights of the model are updated with each time they are updated, such as at the end of each batch of training examples\n",
        "\n",
        "\n",
        "\n",
        "35. How does GD handle local optima in optimization problems?\n",
        "------>\n",
        "A local optimum can be obtained by finding the optimal solution within a neighboring set of candidate solutions. A global optimum can be obtained by finding the optimal solutions among all possible solutions.\n",
        "\n",
        "\n",
        "36. What is Stochastic Gradient Descent (SGD) and how does it differ from GD?\n",
        "---->\n",
        "SGD is stochastic in nature i.e. it picks up a “random” instance of training data at each step and then computes the gradient, making it much faster as there is much fewer data to manipulate at a single time, unlike Batch GD\n",
        "\n",
        "\n",
        "\n",
        "37. Explain the concept of batch size in GD and its impact on training.\n",
        "------>\n",
        "Batch size is important because it affects both the training time and the generalization of the model. A smaller batch size allows the model to learn from each individual example but takes longer to train. A larger batch size trains faster but may result in the model not capturing the nuances in the data.\n",
        "\n",
        "\n",
        "\n",
        "38. What is the role of momentum in optimization algorithms?\n",
        "----->\n",
        "Momentum is an extension to the gradient descent optimization algorithm that allows the search to build inertia in a direction in the search space and overcome the oscillations of noisy gradients and coast across flat spots of the search space.\n",
        "\n",
        "\n",
        "\n",
        "39. What is the difference between batch GD, mini-batch GD, and SGD?\n",
        "------>\n",
        "Batch Gradient Descent can be used for smoother curves. SGD can be used when the dataset is large. Batch Gradient Descent converges directly to minima. SGD converges faster for larger datasets.\n",
        "\n",
        "\n",
        "40. How does the learning rate affect the convergence of GD?\n",
        "------>\n",
        "Pick a value for the learning rate α. The learning rate determines how big the step would be on each iteration. If α is very small, it would take long time to converge and become computationally expensive. If α is large, it may fail to converge and overshoot the minimum\n"
      ],
      "metadata": {
        "id": "GpcmG1__fiFU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Regularization:**\n",
        "\n",
        "41. What is regularization and why is it used in machine learning?\n",
        "------>\n",
        "Why is regularization used in machine learning?\n",
        "Regularization in Machine Learning || Simplilearn\n",
        "Regularization refers to techniques that are used to calibrate machine learning models in order to minimize the adjusted loss function and prevent overfitting or underfitting. Using Regularization, we can fit our machine learning model appropriately on a given test set and hence reduce the errors in it\n",
        "\n",
        "\n",
        "\n",
        "42. What is the difference between L1 and L2 regularization?\n",
        "----->\n",
        "L1 regularization uses the sum of the absolute values of the weights. L2 regularization uses the sum of the squared values of the weights.\n",
        "\n",
        "\n",
        "43. Explain the concept of ridge regression and its role in regularization.\n",
        "----->\n",
        "Ridge regression is a model tuning method that is used to analyse any data that suffers from multicollinearity. This method performs L2 regularization. When the issue of multicollinearity occurs, least-squares are unbiased, and variances are large, this results in predicted values being far away from the actual values.\n",
        "\n",
        "\n",
        "\n",
        "44. What is the elastic net regularization and how does it combine L1 and L2 penalties?\n",
        "----->\n",
        "The elastic net is a linear regression regularization technique that combines both the L1 (Lasso) and L2 (Ridge) regularization penalties. It is particularly useful when dealing with datasets that have high collinearity or when there are more predictors than observations.\n",
        "\n",
        "\n",
        "\n",
        "45. How does regularization help prevent overfitting in machine learning models?\n",
        "----->\n",
        "Regularization is a technique that penalizes the coefficient. In an overfit model, the coefficients are generally inflated. Thus, Regularization adds penalties to the parameters and avoids them weigh heavily. The coefficients are added to the cost function of the linear equation.\n",
        "\n",
        "\n",
        "\n",
        "46. What is early stopping and how does it relate to regularization?\n",
        "------>\n",
        "In machine learning, early stopping is a form of regularization used to avoid overfitting when training a learner with an iterative method, such as gradient descent. Such methods update the learner so as to make it better fit the training data with each iteration.\n",
        "\n",
        "\n",
        "\n",
        "47. Explain the concept of dropout regularization in neural networks.\n",
        "------>\n",
        "Dropout is a regularization method approximating concurrent training of many neural networks with various designs. During training, some layer outputs are ignored or dropped at random. This makes the layer appear and is regarded as having a different number of nodes and connectedness to the preceding layer\n",
        "\n",
        "\n",
        "\n",
        "48. How do you choose the regularization parameter in a model?\n",
        "----->\n",
        "on the training set, we estimate several different Ridge regressions, with different values of the regularization parameter; on the validation set, we choose the best model (the regularization parameter which gives the lowest MSE on the validation set);\n",
        "\n",
        "\n",
        "\n",
        "49. What is the difference between feature selection and regularization?\n",
        "----->\n",
        "Feature selection, also known as feature subset selection, variable selection, or attribute selection. This approach removes the dimensions (e.g. columns) from the input data and results in a reduced data set for model inference. Regularization, where we are constraining the solution space while doing optimization.\n",
        "\n",
        "\n",
        "50. What is the trade-off between bias and variance in regularized models?\n",
        "------->\n",
        "f the algorithm is too simple (hypothesis with linear equation) then it may be on high bias and low variance condition and thus is error-prone. If algorithms fit too complex (hypothesis with high degree equation) then it may be on high variance and low bias\n"
      ],
      "metadata": {
        "id": "638UDyYYhPkL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM:\n",
        "**bold text**\n",
        "\n",
        "51. What is Support Vector Machines (SVM) and how does it work?\n",
        "---->\n",
        " supervised learning models that analyze data and recognize patterns, used for classification and regression analysis [27]. SVM works by constructing hyperplanes in a multidimensional space that separates cases of different class labels.\n",
        "\n",
        "\n",
        "52. How does the kernel trick work in SVM?\n",
        "------->\n",
        "The “trick” is that kernel methods represent the data only through a set of pairwise similarity comparisons between the original data observations x (with the original coordinates in the lower dimensional space), instead of explicitly applying the transformations ϕ(x) and representing the data by these transformed ...\n",
        "\n",
        "\n",
        "53. What are support vectors in SVM and why are they important?\n",
        "------>\n",
        "To create the best line or decision boundary that can segregate n-dimensional space into classes so that we can easily put the new data point in the correct category in the future. This best decision boundary is called a hyperplane\n",
        "\n",
        "\n",
        "54. Explain the concept of the margin in SVM and its impact on model performance.\n",
        "----->\n",
        "We introduced two reasons why SVM needs to find the maximum margin. First, a large margin can avoid the effect of random noise and reduce overfitting. Second, a larger margin will lead to a smaller VC dimension, reduce the number of potential classifiers, and, therefore, reduce the possibility of generalization error.\n",
        "\n",
        "\n",
        "55. How do you handle unbalanced datasets in SVM?\n",
        "----->\n",
        "Perhaps the simplest and most common extension to SVM for imbalanced classification is to weight the C value in proportion to the importance of each class. To accommodate these factors in SVMs an instance-level weighted modification was proposed.\n",
        "\n",
        "\n",
        "\n",
        "56. What is the difference between linear SVM and non-linear SVM?\n",
        "------->\n",
        "Linear SVM: When the data points are linearly separable into two classes, the data is called linearly-separable data. We use the linear SVM classifier to classify such data. Non-linear SVM: When the data is not linearly separable, we use the non-linear SVM classifier to separate the data points.\n",
        "\n",
        "\n",
        "\n",
        "57. What is the role of C-parameter in SVM and how does it affect the decision boundary?\n",
        "---->\n",
        "The Role of C in SVM\n",
        "in SVM helps control the trade-off between the training error and the margin since it can determine the penalty for misclassified data points during the training process. puts more emphasis on minimizing the training error, potentially leading to a narrower margin\n",
        "\n",
        "\n",
        "\n",
        "58. Explain the concept of slack variables in SVM.\n",
        "----->\n",
        "Slack variables are introduced to allow certain constraints to be violated. That is, certain train- ing points will be allowed to be within the margin. We want the number of points within the margin to be as small as possible, and of course we want their penetration of the margin to be as small as possible.\n",
        "\n",
        "\n",
        "\n",
        "59. What is the difference between hard margin and soft margin in SVM?\n",
        "-------->\n",
        "When the data is linearly separable, and we don't want to have any misclassifications, we use SVM with a hard margin. However, when a linear boundary is not feasible, or we want to allow some misclassifications in the hope of achieving better generality, we can opt for a soft margin for our classifier\n",
        "\n",
        "\n",
        "\n",
        "60. How do you interpret the coefficients in an SVM model?-\n",
        "------>\n",
        "Recall that in linear SVM, the result is a hyperplane that separates the classes as best as possible. The weights represent this hyperplane, by giving you the coordinates of a vector which is orthogonal to the hyperplane - these are the coefficients given by svm."
      ],
      "metadata": {
        "id": "BZSvflTBi40j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decision Trees:**\n",
        "\n",
        "\n",
        "61. What is a decision tree and how does it work?\n",
        "------>\n",
        "What is decision tree and how it works?\n",
        "A decision tree is a non-parametric supervised learning algorithm, which is utilized for both classification and regression tasks. It has a hierarchical, tree structure, which consists of a root node, branches, internal nodes and leaf nodes.\n",
        "\n",
        "\n",
        "\n",
        "62. How do you make splits in a decision tree?\n",
        "------>\n",
        "For each split, individually calculate the entropy of each child node. Calculate the entropy of each split as the weighted average entropy of child nodes. Select the split with the lowest entropy or highest information gain\n",
        "\n",
        "\n",
        "\n",
        "63. What are impurity measures (e.g., Gini index, entropy) and how are they used in decision trees?\n",
        "----->\n",
        "Least squares. This method is similar to minimizing least squares in a linear model. ...\n",
        "Least absolute deviations. This method minimizes the mean absolute deviation from the median within a node.\n",
        "\n",
        "\n",
        "64. Explain the concept of information gain in decision trees.\n",
        "------>\n",
        "Information gain is the basic criterion to decide whether a feature should be used to split a node or not. The feature with the optimal split i.e., the highest value of information gain at a node of a decision tree is used as the feature for splitting the node.\n",
        "\n",
        "\n",
        "\n",
        "65. How do you handle missing values in decision trees?\n",
        "------->\n",
        "Surrogate splitting rules enable you to use the values of other input variables to perform a split for observations with missing values. Important Note : Tree Surrogate splitting rule method can impute missing values for both numeric and categorical variables.\n",
        "\n",
        "\n",
        "\n",
        "66. What is pruning in decision trees and why is it important?\n",
        "----->\n",
        "A Decision tree that is trained to its full depth will highly likely lead to overfitting the training data - therefore Pruning is important. In simpler terms, the aim of Decision Tree Pruning is to construct an algorithm that will perform worse on training data but will generalize better on test data.\n",
        "\n",
        "\n",
        "\n",
        "67. What is the difference between a classification tree and a regression tree?\n",
        "----->\n",
        "Classification trees are used when the dataset needs to be split into classes that belong to the response variable. Regression trees, on the other hand, are used when the response variable is continuous\n",
        "\n",
        "\n",
        "\n",
        "68. How do you interpret the decision boundaries in a decision tree?\n",
        "------>\n",
        "To interpret a decision tree, you need to follow the path from the root node to the leaf node that corresponds to your data point or scenario. Each node and branch will tell you what feature and value are used to split the data, and what proportion and value of the outcome variable are associated with each group.\n",
        "\n",
        "\n",
        "69. What is the role of feature importance in decision trees?\n",
        "------>\n",
        "Feature importance is calculated as the decrease in node impurity weighted by the probability of reaching that node. The node probability can be calculated by the number of samples that reach the node, divided by the total number of samples. The higher the value the more important the feature.\n",
        "\n",
        "\n",
        "\n",
        "70. What are ensemble techniques and how are they related to decision trees?\n",
        "----->\n",
        "Using one decision tree is can be problematic and might not be stable enough; however, using multiple decision trees and combining their results will do great. Combining multiple classifiers in a prediction model is called ensembling. The simple rule of ensemble methods is to reduce the error by reducing the variance\n"
      ],
      "metadata": {
        "id": "94bdHS9elFOi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ensemble Techniques:**\n",
        "\n",
        "\n",
        "71. What are ensemble techniques in machine learning?\n",
        "---->\n"
      ],
      "metadata": {
        "id": "79UMMYzZm93a"
      }
    }
  ]
}