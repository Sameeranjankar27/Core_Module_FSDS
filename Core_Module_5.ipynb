{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Naive Approach:**\n",
        "\n",
        "1. What is the Naive Approach in machine learning?\n",
        ">\n",
        "A naive classifier model is one that does not use any sophistication in order to make a prediction, typically making a random or constant prediction. Such models are naive because they don't use any knowledge about the domain or any learning in order to make a prediction.\n",
        "\n",
        "\n",
        "2. Explain the assumptions of feature independence in the Naive Approach.\n",
        ">\n",
        "The assumption of feature independence in Naive Bayes simplifies the computation and makes the algorithm more efficient. By assuming independence, the joint probability of features given the class can be computed as the product of individual feature probabilities\n",
        "\n",
        "\n",
        "3. How does the Naive Approach handle missing values in the data?\n",
        ">\n",
        "14\n",
        "\n",
        "Naive Bayes apparently handles missing data differently, depending on whether they exist in training or testing/classification instances.\n",
        "\n",
        "When classifying instances, the attribute with the missing value is simply not included in the probability calculation\n",
        "\n",
        "In training, the instance [with the missing data] is not included in frequency count for attribute value-class combinatio\n",
        "\n",
        "\n",
        "4. What are the advantages and disadvantages of the Naive Approach?\n",
        ">\n",
        "Advantages\n",
        "This algorithm works quickly and can save a lot of time.\n",
        "Naive Bayes is suitable for solving multi-class prediction problems.\n",
        "If its assumption of the independence of features holds true, it can perform better than other models and requires much less training data.\n",
        "Naive Bayes is better suited for categorical input variables than numerical variables.\n",
        "\n",
        "\n",
        "Disadvantages\n",
        "Naive Bayes assumes that all predictors (or features) are independent, rarely happening in real life. This limits the applicability of this algorithm in real-world use cases.\n",
        "This algorithm faces the ‘zero-frequency problem’ where it assigns zero probability to a categorical variable whose category in the test data set wasn’t available in the training dataset. It would be best if you used a smoothing technique to overcome this issue.\n",
        "Its estimations can be wrong in some cases, so you shouldn’t take its probability outputs very seriously.\n",
        "\n",
        "\n",
        "5. Can the Naive Approach be used for regression problems? If yes, how?\n",
        ">\n",
        "Naive Bayes is a supervised classification algorithm that is used primarily for dealing with binary and multi-class classification problems, though with some modifications, it can also be used for solving regression problems.\n",
        "\n",
        "\n",
        "6. How do you handle categorical features in the Naive Approach?\n",
        ">\n",
        "Step 1: Drop columns with categorical data. You'll get started with the most straightforward approach. ...\n",
        "Step 2: Label encoding. Before jumping into label encoding, we'll investigate the dataset. ...\n",
        "Step 3: Investigating cardinality. ...\n",
        "Step 4: One-hot encoding.\n",
        "\n",
        "\n",
        "7. What is Laplace smoothing and why is it used in the Naive Approach?\n",
        ">\n",
        "Laplace smoothing is a smoothing technique that helps tackle the problem of zero probability in the Naïve Bayes machine learning algorithm. Using higher alpha values will push the likelihood towards a value of 0.5, i.e., the probability of a word equal to 0.5 for both the positive and negative reviews\n",
        "\n",
        "\n",
        "8. How do you choose the appropriate probability threshold in the Naive Approach?\n",
        ">\n",
        "The default value for the minimum threshold of probabilities is 0.001. Valid values are positive numbers that are smaller than 1. The Naive Bayes method can only work with discrete input fields.\n",
        "\n",
        "\n",
        "\n",
        "9. Give an example scenario where the Naive Approach can be applied.\n",
        ">\n",
        "What are the examples of Naive Bayes application?\n",
        "Naive Bayes Algorithm in ML: Simplifying Classification Problems\n",
        "Some best examples of the Naive Bayes Algorithm are sentimental analysis, classifying new articles, and spam filtration. Classification algorithms are used for categorizing new observations into predefined classes for the uninitiated data. The Naive Bayes Algorithm is known for its simplicity and effectiveness\n",
        "\n"
      ],
      "metadata": {
        "id": "emn917yXn0YU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**KNN:**\n",
        "\n",
        "\n",
        "10. What is the K-Nearest Neighbors (KNN) algorithm?\n",
        ">\n",
        "What is K nearest neighbors algorithm? A. KNN classifier is a machine learning algorithm used for classification and regression problems. It works by finding the K nearest points in the training dataset and uses their class to predict the class or value of a new data point.\n",
        "\n",
        "\n",
        "11. How does the KNN algorithm work?\n",
        ">\n",
        "First, the distance between the new point and each training point is calculated.\n",
        "The closest k data points are selected (based on the distance). ...\n",
        "The average of these data points is the final prediction for the new point.\n",
        "\n",
        "\n",
        "\n",
        "12. How do you choose the value of K in KNN?\n",
        ">\n",
        "The value of k is very crucial in the KNN algorithm to define the number of neighbors in the algorithm. The value of k in the k-nearest neighbors (k-NN) algorithm should be chosen based on the input data. If the input data has more outliers or noise, a higher value of k would be better.\n",
        "\n",
        "\n",
        "\n",
        "13. What are the advantages and disadvantages of the KNN algorithm?\n",
        ">\n",
        "Advantages\t                         Disadvantages\n",
        "Simple and Easy to Understand  \t    Sensitive to Outliers\n",
        "Non-parametric\tComputationally      Expensive\n",
        "No Training Required\t               Requires Good Choice of K\n",
        "Can Handle Large Datasets      \t    Limited to Euclidean Distance\n",
        "\n",
        "\n",
        "\n",
        "14. How does the choice of distance metric affect the performance of KNN?\n",
        ">\n",
        "In addition, the performance of the KNN with this top performing distance degraded only ∼20% while the noise level reaches 90%, this is true for most of the distances used as well. This means that the KNN classifier using any of the top 10 distances tolerates noise to a certain degree.\n",
        "\n",
        "\n",
        "\n",
        "15. Can KNN handle imbalanced datasets? If yes, how?\n",
        ">\n",
        "In k Nearest Neighbor (kNN) classifier, a query instance is classified based on the most frequent class of its nearest neighbors among the training instances. In imbalanced datasets, kNN becomes biased towards the majority instances of the training space.\n",
        "\n",
        "\n",
        "\n",
        "16. How do you handle categorical features in KNN?\n",
        ">\n",
        "You have to decide how to convert categorical features to a numeric scale, and somehow assign inter-category distances in a way that makes sense with other features (like, age-age distances...but what is an age-category distance?)\n",
        "\n",
        "\n",
        "\n",
        "17. What are some techniques for improving the efficiency of KNN?\n",
        ">\n",
        "In order to improve the efficiency and speed of KNN, reducing the dimensionality of the data is necessary. The curse of dimensionality can make the distance between data points less distinguishable and increase complexity for the model.\n",
        "\n",
        "\n",
        "\n",
        "18. Give an example scenario where KNN can be applied.\n",
        ">\n",
        "Many companies make a personalized recommendation for its consumers, such as Netflix, Amazon, YouTube, and many more. KNN can search for semantically similar documents. Each document is considered as a vector\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_lpm2yoRpbBs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clustering**\n",
        "\n",
        "\n",
        "19. What is clustering in machine learning?\n",
        ">\n",
        "Clustering is the task of dividing the population or data points into a number of groups such that data points in the same groups are more similar to other data points in the same group and dissimilar to the data points in other groups.\n",
        "\n",
        "\n",
        "20. Explain the difference between hierarchical clustering and k-means clustering.\n",
        ">\n",
        "k-means is method of cluster analysis using a pre-specified no. of clusters. It requires advance knowledge of 'K'. Hierarchical clustering also known as hierarchical cluster analysis (HCA) is also a method of cluster analysis which seeks to build a hierarchy of clusters without having fixed number of cluster.\n",
        "\n",
        "\n",
        "21. How do you determine the optimal number of clusters in k-means clustering?\n",
        ">\n",
        "Compute clustering algorithm (e.g., k-means clustering) for different values of k. ...\n",
        "For each k, calculate the total within-cluster sum of square (wss).\n",
        "Plot the curve of wss according to the number of clusters k.\n",
        "More items...\n",
        "\n",
        "\n",
        "22. What are some common distance metrics used in clustering?\n",
        ">\n",
        "Distance metrics are used in supervised and unsupervised learning to calculate similarity in data points. They improve the performance, whether that's for classification tasks or clustering. The four types of distance metrics are Euclidean Distance, Manhattan Distance, Minkowski Distance, and Hamming Distance.\n",
        "\n",
        "\n",
        "\n",
        "23. How do you handle categorical features in clustering?\n",
        ">\n",
        "One way to handle categorical variables is to use one-hot encoding. One-hot encoding transforms categorical variables into a set of binary features, where each feature represents a distinct category. For example, suppose we have a categorical variable “color” that can take on the values red, blue, or yellow.\n",
        "\n",
        "\n",
        "\n",
        "24. What are the advantages and disadvantages of hierarchical clustering?\n",
        ">\n",
        "The advantage of Hierarchical Clustering is we don't have to pre-specify the clusters. However, it doesn't work very well on vast amounts of data or huge datasets. And there are some disadvantages of the Hierarchical Clustering algorithm that it is not suitable for large datasets\n",
        "\n",
        "\n",
        "\n",
        "25. Explain the concept of silhouette score and its interpretation in clustering.\n",
        ">\n",
        "Silhouette (clustering)\n",
        "The silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The silhouette ranges from −1 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters.\n",
        "\n",
        "\n",
        "\n",
        "26. Give an example scenario where clustering can be applied.\n",
        ">\n",
        "For instance, clustering can be used to identify several types of depression. Cluster analysis is also used to identify patterns in the spatial or temporal allocation of a disease. Business − Businesses collect huge amounts of data on current and potential users.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "J7iN3hjdq0wp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Anomaly Detection:**\n",
        "\n",
        "\n",
        "27. What is anomaly detection in machine learning?\n",
        ">\n",
        "Anomaly detection (or outlier detection) is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data.\n",
        "\n",
        "\n",
        "28. Explain the difference between supervised and unsupervised anomaly detection.\n",
        ">\n",
        "Supervised learning harnesses the power of labeled data to train models that can make accurate predictions or classifications. In contrast, unsupervised learning focuses on uncovering hidden patterns and structures within unlabeled data, using techniques like clustering or anomaly detection.\n",
        "\n",
        "\n",
        "29. What are some common techniques used for anomaly detection?\n",
        ">\n",
        "Some of the popular techniques are: Statistical (Z-score, Tukey's range test and Grubbs's test) Density-based techniques (k-nearest neighbor, local outlier factor, isolation forests, and many more variations of this concept) Subspace-, correlation-based and tensor-based outlier detection for high-dimensional data.\n",
        "\n",
        "\n",
        "30. How does the One-Class SVM algorithm work for anomaly detection?\n",
        ">\n",
        "One-class SVM, or unsupervised SVM, is an algorithm used for anomaly detection. The algorithm tries to separate data from the origin in the transformed high-dimensional predictor space. ocsvm finds the decision boundary based on the primal form of SVM with the Gaussian kernel approximation method.\n",
        "\n",
        "\n",
        "\n",
        "31. How do you choose the appropriate threshold for anomaly detection?\n",
        ">\n",
        "There are three main classes of anomaly detection techniques: unsupervised, semi-supervised, and supervised. Essentially, the correct anomaly detection method depends on the available labels in the dataset.\n",
        "\n",
        "\n",
        "32. How do you handle imbalanced datasets in anomaly detection?\n",
        ">\n",
        "Choose Proper Evaluation Metric.\n",
        "Resampling (Oversampling and Undersampling)\n",
        "SMOTE.\n",
        "BalancedBaggingClassifier.\n",
        "\n",
        "\n",
        "\n",
        "33. Give an example scenario where anomaly detection can be applied.\n",
        ">\n",
        "a credit card company will use anomaly detection to track how customers typically use their credit cards. If a customer makes an abnormally large purchase or a purchase in a new location, the algorithm recognizes the anomaly and alerts a team member to contact the customer\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sF0ZSB-br4qH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dimension Reduction:**\n",
        "\n",
        "\n",
        "34. What is dimension reduction in machine learning?\n",
        ">\n",
        "Dimensionality reduction is a technique used to reduce the number of features in a dataset while retaining as much of the important information as possible. In other words, it is a process of transforming high-dimensional data into a lower-dimensional space that still preserves the essence of the original data.\n",
        "\n",
        "\n",
        "\n",
        "35. Explain the difference between feature selection and feature extraction.\n",
        ">\n",
        "The key difference between feature selection and feature extraction techniques used for dimensionality reduction is that while the original features are maintained in the case of feature selection algorithms, the feature extraction algorithms transform the data onto a new feature space\n",
        "\n",
        "\n",
        "\n",
        "36. How does Principal Component Analysis (PCA) work for dimension reduction?\n",
        ">\n",
        "Principal component analysis, or PCA, is a dimensionality reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "37. How do you choose the number of components in PCA?\n",
        ">\n",
        "If our sole intention of doing PCA is for data visualization, the best number of components is 2 or 3. If we really want to reduce the size of the dataset, the best number of principal components is much less than the number of variables in the original dataset\n",
        "\n",
        "\n",
        "\n",
        "38. What are some other dimension reduction techniques besides PCA?\n",
        ">\n",
        "Dimensionality reduction involves reducing the number of input variables or columns in modeling data. PCA is a technique from linear algebra that can be used to automatically perform dimensionality reduction.\n",
        "\n",
        "\n",
        "39. Give an example scenario where dimension reduction can be applied.\n",
        ">\n",
        "Dimensionality reduction finds a lower number of variables or removes the least important variables from the model. That will reduce the model's complexity and also remove some noise in the data. In this way, dimensionality reduction helps to mitigate overfitting.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2jQetcYKsv2x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature Selection:**\n",
        "\n",
        "\n",
        "40. What is feature selection in machine learning?\n",
        ">\n",
        "Feature Selection is the method of reducing the input variable to your model by using only relevant data and getting rid of noise in data. It is the process of automatically choosing relevant features for your machine learning model based on the type of problem you are trying to solve.\n",
        "\n",
        "\n",
        "\n",
        "41. Explain the difference between filter, wrapper, and embedded methods of feature selection.\n",
        ">\n",
        "Filter method is faster and useful when there are more number of features. Wrapper method gives better performance while the embedded method lies in between the other two methods.\n",
        "\n",
        "\n",
        "\n",
        "42. How does correlation-based feature selection work?\n",
        ">\n",
        "CFS (Correlation based Feature Selection) is an algorithm that couples this evaluation formula with an appropriate correlation measure and a heuristic search strategy. CFS was evaluated by experiments on artificial and natural datasets. Three machine learning algorithms were used: C4.\n",
        "\n",
        "\n",
        "\n",
        "43. How do you handle multicollinearity in feature selection?\n",
        ">\n",
        "Removing variables. A straightforward method of correcting multicollinearity is removing one or more variables showing a high correlation. ...\n",
        "More data. ...\n",
        "Using techniques such as partial least squares regression (PLS) and principal component analysis (PCA). ...\n",
        "Centering the variables.\n",
        "\n",
        "\n",
        "\n",
        "44. What are some common feature selection metrics?\n",
        ">\n",
        "There are three general classes of feature selection algorithms: Filter methods, wrapper methods and embedded methods. The role of feature selection in machine learning is, 1. To reduce the dimensionality of feature space\n",
        "\n",
        "\n",
        "\n",
        "45. Give an example scenario where feature selection can be applied.\n",
        ">\n",
        "Mammographic image analysis. Criminal behavior modeling. Genomic data analysis.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cGpM_ggVtoD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Leakage:**\n",
        "\n",
        "\n",
        "51. What is data leakage in machine learning?\n",
        ">\n",
        "Data leakage is a big problem in machine learning when developing predictive models.\n",
        "\n",
        "Data leakage is when information from outside the training dataset is used to create the model.\n",
        "\n",
        "In this post you will discover the problem of data leakage in predictive modeling.\n",
        "\n",
        "After reading this post you will know:\n",
        "\n",
        "What is data leakage is in predictive modeling.\n",
        "Signs of data leakage and why it is a problem.\n",
        "Tips and tricks that you can use to minimize data leakage on your predictive modeling problems\n",
        "\n",
        "\n",
        "\n",
        "52. Why is data leakage a concern?\n",
        ">\n",
        "The unauthorized transmission of data from an organization to any external source is known as data leakage. This data can be leaked physically or electronically via hard drives, USB devices, mobile phones, etc., and could be exposed publicly or fall into the hands of a cyber criminal.\n",
        "\n",
        "\n",
        "\n",
        "53. Explain the difference between target leakage and train-test contamination.\n",
        ">\n",
        " happens when your training data contains information about the target, but similar data will not be available when the model is used for prediction. This leads to high performance on the training set (and possibly even the validation data), but the model will perform poorly in production.\n",
        "\n",
        "\n",
        "\n",
        " 54. How can you identify and prevent data leakage in a machine learning pipeline?\n",
        " >\n",
        " One of the best ways to get rid of data leakage is to perform k-fold cross validation where the overall data is divided into k parts. After dividing into k parts, we use each part as the cross-validation data and the remaining as training data\n",
        "\n",
        "\n",
        "\n",
        " 55. What are some common sources of data leakage?\n",
        " >\n",
        " oor data security and sanitization, outdated systems, or a lack of employee training. Data leaks could lead to identity theft, data breaches, or ransomware installation.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "G0qbgPjcuXRf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cross Validation:**\n",
        "\n",
        "\n",
        "57. What is cross-validation in machine learning?\n",
        ">\n",
        "Cross validation is a technique used in machine learning to evaluate the performance of a model on unseen data. It involves dividing the available data into multiple folds or subsets, using one of these folds as a validation set, and training the model on the remaining folds.\n",
        "\n",
        "\n",
        "58. Why is cross-validation important?\n",
        ">\n",
        "The purpose of cross–validation is to test the ability of a machine learning model to predict new data. It is also used to flag problems like overfitting or selection bias and gives insights on how the model will generalize to an independent dataset.\n",
        "\n",
        "\n",
        "\n",
        "59. Explain the difference between k-fold cross-validation and stratified k-fold cross-validation.\n",
        ">\n",
        "KFold devides the dataset into k folds. Where as Stratified ensures that each fold of dataset has the same proportion of observations with a given label.\n",
        "\n",
        "\n",
        "\n",
        "60. How do you interpret the cross-validation results?\n",
        ">\n",
        "What is 5 fold cross-validation interpretation?\n",
        "K-fold Cross-Validation is when the dataset is split into a K number of folds and is used to evaluate the model's ability when given new data. K refers to the number of groups the data sample is split into. For example, if you see that the k-value is 5, we can call this a 5-fold cross-validation"
      ],
      "metadata": {
        "id": "WbdumVMevInp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9jz_hXsNvt3h"
      }
    }
  ]
}