{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1.How do word embeddings capture semantic meaning in text preprocessing?\n",
        "> Word Embeddings in NLP is a technique where individual words are represented as real-valued vectors in a lower-dimensional space and captures inter-word semantics. Each word is represented by a real-valued vector with tens or hundreds of dimensions.\n",
        "\n",
        "\n",
        "2. Explain the concept of recurrent neural networks (RNNs) and their role in text processing tasks.\n",
        "> A recurrent neural network is a type of artificial neural network commonly used in speech recognition and natural language processing. Recurrent neural networks recognize data's sequential characteristics and use patterns to predict the next likely scenario.\n",
        "\n",
        "3. What is the encoder-decoder concept, and how is it applied in tasks like machine translation or text summarization?\n",
        "> Encoder-decoder architecture is used in machine translation, text summarization, and image captioning tasks. Encoder compresses input to fixed-length vector; decoder generates output from it. It can be implemented using RNNs or transformer networks and trained using input-output pairs to learn to map.\n",
        "\n",
        "4. Discuss the advantages of attention-based mechanisms in text processing models.\n",
        "> The attention mechanism allows the model to \"pay attention\" to certain parts of the data and to give them more weight when making predictions. In a nutshell, the attention mechanism helps preserve the context of every word in a sentence by assigning an attention weight relative to all other words.\n",
        "\n",
        "5. Explain the concept of self-attention mechanism and its advantages in natural language processing.\n",
        "> Self-Attention. The attention mechanism allows output to focus attention on input while producing output while the self-attention model allows inputs to interact with each other (i.e calculate attention of all other inputs wrt one input.\n",
        "\n",
        "6. What is the transformer architecture, and how does it improve upon traditional RNN-based models in text processing?\n",
        "> Transformers were introduced in the context of machine translation with the purpose to avoid recursion in order to allow parallel computation (to reduce training time) and also to reduce drops in performance due to long dependencies.\n",
        "The first property is the reason why RNN and LSTM can't be trained in parallel. In order to encode the second word in a sentence I need the previously computed hidden states of the first word, therefore I need to compute that first. The second property is a bit more subtle, but not hard to grasp conceptually. Information in RNN and LSTM are retained thanks to previously computed hidden states. The point is that the encoding of a specific word is retained only for the next time step, which means that the encoding of a word strongly affects only the representation of the next word, so its influence is quickly lost after a few time steps\n",
        "\n",
        "\n",
        "7. Describe the process of text generation using generative-based approaches.\n",
        "> These models are trained on large amounts of text data and use statistical techniques to predict the likelihood of a word or sequence of words given the context. The model can then generate text by sampling from this distribution of words and selecting the most likely words based on the context.\n",
        "\n",
        "8. What are some applications of generative-based approaches in text processing?\n",
        "> Potential applications: Data augmentation, dataset synthesis, art creation, code generation, text generation, audio synthesis, etc. Synthesized by score-based generative models.\n",
        "\n",
        "9. Discuss the challenges and techniques involved in building conversation AI systems.\n",
        "> Regional jargon and slang.\n",
        "Dialects not conforming to standard language.\n",
        "Background noise distorting the voice of the speaker.\n",
        "Unscripted questions that the virtual assistant or chatbot does not know to answer.\n",
        "Unplanned responses by customers.\n",
        "\n",
        "10. How do you handle dialogue context and maintain coherence in conversation AI models?\n",
        "> When building a chatbot, developing, managing and fine-tuning the dialog flow state is important. Certain conversational AI elements are really intended to be used as the backbone of the dialog, others should be used in a supporting capacity, or only for specific use-cases.\n",
        "There are 5 broad approaches to managing a conversation:\n",
        "1.Intents, Entities, Dialog State Systems & Bot Messages\n",
        "2.ML Stories\n",
        "3.Chitchat\n",
        "4.Large Language Models\n",
        "5.Knowledge Base Management\n",
        "\n",
        "11. Explain the concept of intent recognition in the context of conversation AI.\n",
        "> Intent recognition is the process of identifying and understanding a user's intention or goal behind a given text or speech input in a conversational AI system.\n",
        "\n",
        "12. Discuss the advantages of using word embeddings in text preprocessing.\n",
        "> In Agent Assist, Word Embeddings help us understand the meaning of each word, which can be used to recommend articles, suggest automations, and enable more features based on the dialogue meaning.\n",
        "\n",
        "13. How do RNN-based techniques handle sequential information in text processing tasks?\n",
        "> In text classification the prediction of the network is to classify which group or groups the text belongs to. A common use is classifying if the sentiment of a piece of text is positive or negative.\n",
        "If an RNN is trained to predict text from a corpus within a given domain as in the RNN explanation earlier in this article, it is close to ideal to be re-purposed for text classification within that domain. The generation ‘head’ of the network is removed leaving the ‘backbone’ of the network. The weights within the backbone can then be frozen. A new classification head can then be attached to the backbone and trained to predict the required classifications.\n",
        "It can be a very effective method to speed up training to gradually unfreeze the weights within the layers. Starting with the weights of the last two layers, then the weights of the last three layers, and finally all unfreeze all of the layers’ weights.\n",
        "\n",
        "14. What is the role of the encoder in the encoder-decoder architecture?\n",
        "> Encoder-decoder architecture is used in machine translation, text summarization, and image captioning tasks. Encoder compresses input to fixed-length vector; decoder generates output from it. It can be implemented using RNNs or transformer networks and trained using input-output pairs to learn to map.\n",
        "\n",
        "15. Explain the concept of attention-based mechanism and its significance in text processing.\n",
        "> Attention mechanism is one of the recent advancements in Deep learning especially for Natural language processing tasks like Machine translation, Image Captioning, dialogue generation etc. It is a mechanism that is developed to increase the performance of encoder decoder(seq2seq) RNN model.\n",
        "\n",
        "16. How does self-attention mechanism capture dependencies between words in a text?\n",
        "> The Transformer uses the self-attention mechanism where attention weights are calculated using all the words in the input sequence at once, hence it facilitates parallelization.\n",
        "\n",
        "17. Discuss the advantages of the transformer architecture over traditional RNN-based models.\n",
        "> transformers are faster than RNN-based models as all the input is ingested once. Training LSTMs is harder when compared with transformer networks, since the number of parameters is a lot more in LSTM networks. Moreover, it's impossible to do transfer learning in LSTM networks.\n",
        "\n",
        "18. What are some applications of text generation using generative-based approaches?\n",
        "> What are the applications of generative models? Generative models are a set of algorithms that generate data, usually in the form of text or images. They can be used to create new content, such as machine-generated poetry or fake news, or to model the underlying structure of existing data\n",
        "\n",
        "19. How can generative models be applied in conversation AI systems?\n",
        "> Over the last couple of years many of us have been surprised again and again by Generative AI models and LLMs in particular. A few years ago, it wasn't possible to automatically turn a few bullet points into a short essay. Now it’s becoming a table-stakes feature for writing apps. LLMs have serious limitations which mean they are almost never appropriate to use without a human in the loop. But they can still improve time-to-value for mission-critical conversational AI applications.\n",
        "\n",
        "\n",
        "20. Explain the concept of natural language understanding (NLU) in the context of conversation AI.\n",
        "> NLU enables human-computer interaction. It is the comprehension of human language such as English, Spanish and French, for example, that allows computers to understand commands without the formalized syntax of computer languages. NLU also enables computers to communicate back to humans in their own languages.\n",
        "\n",
        "21. What are some challenges in building conversation AI systems for different languages or domains?\n",
        "> Regional jargon and slang.\n",
        "Dialects not conforming to standard language.\n",
        "Background noise distorting the voice of the speaker.\n",
        "Unscripted questions that the virtual assistant or chatbot does not know to answer.\n",
        "Unplanned responses by customers.\n",
        "\n",
        "22. Discuss the role of word embeddings in sentiment analysis tasks.\n",
        "> Word embeddings or distributed representations of words are being used in various applications like machine translation, sentiment analysis, topic identification etc. Quality of word embeddings and performance of their applications depends on several factors like training method, corpus size and relevance etc.\n",
        "\n",
        "23. How do RNN-based techniques handle long-term dependencies in text processing?\n",
        "> Long Short Term Memory networks (LSTMs) is a special kind of recurrent neural network capable of learning long-term dependencies. They were introduced by Hochreiter & Schmidhuber in 1997. Remembering information for longer periods of time is their default behavior. The Long short-term memory (LSTM) is made up of a memory cell, an input gate, an output gate and a forget gate. The memory cell is responsible for remembering the previous state while the gates are responsible for controlling the amount of memory to be exposed.\n",
        "\n",
        "24. Explain the concept of sequence-to-sequence models in text processing tasks.\n",
        "> Seq2Seq (Sequence-to-Sequence) is a type of model in machine learning that is used for tasks such as machine translation, text summarization, and image captioning. The model consists of two main components: Encoder. Decoder.\n",
        "\n",
        "25. What is the significance of attention-based mechanisms in machine translation tasks?\n",
        "> In machine translation, attention mechanism is used to align and selectively focus on relevant parts of the source sentence during the translation process. It allows the model to assign weights to more important words or phrases.\n",
        "\n",
        "26. Discuss the challenges and techniques involved in training generative-based models for text generation.\n",
        "> Text generation with GPT-3 and BERT may be limited by challenges such as data availability and quality, model complexity and scalability, and ethical and social implications. The models rely on large amounts of text data to learn and generate natural language, but the data may contain errors or biases\n",
        "\n",
        "27. How can conversation AI systems be evaluated for their performance and effectiveness?\n",
        ">\n",
        "1. Qualitative. Comprehension Level. User Feedback. Satisfaction and Evaluation Rates. Self-Service Rate.\n",
        "2.Quantitative. Activity Volume. Bounce, Retention, and Goal Completion Rates. Number of Conversations. Customer Support Savings.\n",
        "\n",
        "28. Explain the concept of transfer learning in the context of text preprocessing.\n",
        "> Transfer learning is the application of knowledge gained from completing one task to help solve a different, but related, problem\n",
        "\n",
        "29. What are some challenges in implementing attention-based mechanisms in text processing models?\n",
        "> I would say that the main disadvantage of the attention mechanism is that it adds more weight parameters to the model, which can increase training time especially if the input data for the model are long sequences.\n",
        "In a sequence-to-sequence model (e.g. RNN-based encoder-decoder), the attention mechanism combines the encoder’s output at each time step, along with the decoder’s output at time step t, to create the context vector for time step t. The context vector encapsulates the most relevant information from the encoder, which is why attention is a useful mechanism\n",
        "\n",
        "30. Discuss the role of conversation AI in enhancing user experiences and interactions on social media platforms.\n",
        "> Conversational AI works by combining natural language processing (NLP) and machine learning (ML) processes with conventional, static forms of interactive technology, such as chatbots. This combination is used to respond to users through interactions that mimic those with typical human agents."
      ],
      "metadata": {
        "id": "8xIU_JK6ScMB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "iPaWrDg0V4Gi"
      }
    }
  ]
}